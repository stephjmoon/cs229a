{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT TOKENIZED DATA\n",
    "\n",
    "import csv\n",
    "with open('clean_fake_titles1d.csv', 'r', encoding='utf-8' ) as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_fake_titles_all = list(reader)\n",
    "    clean_fake_titles1d = clean_fake_titles_all[0] \t# output from reader is list of list\n",
    "\n",
    "with open('clean_fake_texts1d.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_fake_texts_all = list(reader)\n",
    "    clean_fake_texts1d = clean_fake_texts_all[0] \t# output from reader is list of list\n",
    "\n",
    "with open('clean_real_texts1d.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_real_texts_all = list(reader)\n",
    "    clean_real_textss1d = clean_real_texts_all[0] \t# output from reader is list of list\n",
    "\n",
    "with open('clean_real_titles1d.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_real_titles_all = list(reader)\n",
    "    clean_real_titles1d = clean_real_titles_all[0] \t# output from reader is list of list\n",
    "\n",
    "with open('clean_fake_texts.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_fake_texts = list(reader)\n",
    "with open('clean_real_texts.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_real_texts = list(reader)\n",
    "with open('clean_fake_titles.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_fake_titles = list(reader)\n",
    "with open('clean_real_titles.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    clean_real_titles= list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DETERMINE FEATURES\n",
    "import numpy as np # linear algebra\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "## User defined parameters\n",
    "num_text_features = 500\n",
    "num_title_features = 100\n",
    "text_gram_size = 3\n",
    "title_gram_size = 1\n",
    "\n",
    "## Extract n-gram features from titles\n",
    "\n",
    "title_grams = nltk.ngrams(clean_fake_titles1d,title_gram_size) # get all n-grams\n",
    "counter = Counter(title_grams) # count number of instances of each n-gram\n",
    "title_features =  []\n",
    "# get the top occuring n-gram to set as features\n",
    "for ng_val,ng_count in counter.most_common(num_title_features):\n",
    "    title_features.append(ng_val)\n",
    "\n",
    "## Extract n-gram features from texts\n",
    "text_grams = nltk.ngrams(clean_fake_texts1d,text_gram_size) # get all n-grams\n",
    "counter = Counter(text_grams) # count number of instances of each n-gram\n",
    "text_features =  []\n",
    "# get the top occuring n-gram to set as features\n",
    "for ng_val,ng_count in counter.most_common(num_text_features):\n",
    "    text_features.append(ng_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(title_features))\n",
    "print(len(text_features))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count number of n-gram features from vocab list in dataset\n",
    "def countNgrams(examples, n_gram_features):\n",
    "    num_examples = len(examples)\n",
    "    num_features = len(n_gram_features)\n",
    "    gram_size = len(n_gram_features[0])\n",
    "    X = np.array([ ([0] * num_features) for row in range(num_examples)])\n",
    "\n",
    "    for i in range(0, num_examples): # for each example (row)\n",
    "        ng_list = list(nltk.ngrams(examples[i],gram_size)) # get all n-grams\n",
    "\n",
    "        # fill X with how many occurances of n_gram_features occur from the example\n",
    "        for ng in ng_list:\n",
    "            if ng in n_gram_features:\n",
    "                X[i][n_gram_features.index(ng)] += 1 \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "X_fake_title = countNgrams(clean_fake_titles1d, title_features)\n",
    "X_real_title = countNgrams(clean_real_titles1d, title_features)\n",
    "X_fake_text = countNgrams(clean_fake_texts1d, text_features)\n",
    "X_real_text = countNgrams(clean_real_texts1d, text_features)\n",
    "    \n",
    "# CONSTRUCTING X\n",
    "# num_examples = len(cleantitles)\n",
    "# X = np.array([ ([0] * num_features) for row in range(num_examples)])\n",
    "\n",
    "# for i in range(0, num_examples): # for each example (row)\n",
    "#     ng_example = list(nltk.ngrams(cleantitles[i],gram_size)) # get all n-grams\n",
    "\n",
    "#     # fill X with how many occurances of n_gram_features occur from the example\n",
    "#     for ng in ng_example:\n",
    "#         if ng in n_gram_features:\n",
    "#             X[i][n_gram_features.index(ng)] += 1\n",
    "\n",
    "#print(X.shape) # check that it's a matrix of nunm_examples x num_features\n",
    "\n",
    "# save X\n",
    "#numpy.savetxt(r\"C:\\Users\\steph\\Desktop\\CS 229a\\Project\\X_fake_titles.csv\", X, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_fake_title.shape)\n",
    "print(X_real_title.shape)\n",
    "print(X_fake_text.shape)\n",
    "print(X_real_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"X_fake_title.csv\",\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(X_fake_title)\n",
    "\n",
    "with open (\"X_real_title.csv\",\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(X_real_title)\n",
    "    \n",
    "with open (\"X_fake_text.csv\",\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(X_fake_text)\n",
    "\n",
    "with open (\"X_real_text.csv\",\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(X_real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine title and text matrices\n",
    "X_fake = np.hstack(X_fake_title, X_fake_text)\n",
    "X_real = np.hstack(X_real_title, X_real_text)\n",
    "\n",
    "# Add output to the X matrix\n",
    "y_fake = [[1] * X_fake.shape[0]]\n",
    "y_real = [[1] * X_real.shape[0]]\n",
    "\n",
    "X_fake_complete = np.hstack(X_fake,y_fake)\n",
    "X_real_complete = np.hstack(X_real,y_real)\n",
    "\n",
    "print(y_fake.shape)\n",
    "## Append classification output to the matrix\n",
    "#X_fake_title = np.vstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fake_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Concatenate title and text tokens. Then mix the real and fake data, with corresponding labels.\n",
    "\n",
    "m_real = len(clean_real_titles) # number of examples\n",
    "Xreal = []\n",
    "yreal = []\n",
    "for i in range(0, m_real):\n",
    "    Xreal[i] = clean_real_titles[i] + clean_real_texts[i]\n",
    "\n",
    "m_fake = len(clean_fake_titles)\n",
    "Xfake = []\n",
    "for i in range(0, m_fake):\n",
    "    Xfake[i] = clean_real_titles[i] + clean_real_texts[i]\n",
    "\n",
    "    \n",
    "\n",
    "x = [[i] for i in range(10)]\n",
    "shuffle(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## testing drop na\n",
    "\n",
    "na_bool_title = fake_all['title'].isna()\n",
    "na_bool_text = fake_all['text'].isna()\n",
    "na_bool = (na_bool_title == 1) | (na_bool_text == 1)\n",
    "\n",
    "print(na_bool_title[0:10])\n",
    "print(na_bool_text[0:10])\n",
    "print(na_bool[0:10])\n",
    "\n",
    "# read data and convert to list\n",
    "\n",
    "na_bool_title = fake_all['title'].isna()\n",
    "#na_bool_title_list = na_bool_title.toList()\n",
    "na_bool_text = fake_all['text'].isna()\n",
    "#na_bool_text_list = na_bool_text_toList()\n",
    "\n",
    "na_bool = (na_bool_title == 1) | (na_bool_text == 1)\n",
    "\n",
    "\n",
    "na_index = na_bool.where(na_bool == 1)\n",
    "find_true = na_index.where(na_index != NaN)\n",
    "fake_all.head(13)\n",
    "\n",
    "print(na_bool_title[10:20])\n",
    "print(na_bool_text[10:20])\n",
    "print(na_bool[10:20])\n",
    "print(na_index[10:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
